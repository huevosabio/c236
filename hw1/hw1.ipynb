{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 236: Deep Generative Models\n",
    "#### Ramon Iglesias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{argmax}_\\theta \\mathbb{E}[\\text{log}p_\\theta(y | x)] & = \\text{argmin}_\\theta \\mathbb{E}_{\\hat{p}(x)}[D_{KL}(\\hat{p}(y | x) || p_\\theta(y | x)] \\\\\n",
    "&= \\text{argmin}_\\theta \\mathbb{E}_{\\hat{p}(x)}[\\mathbb{E}_{\\hat{p}(y | x)}[\\text{log}\\hat{p}(y | x) - \\text{log}p_\\theta(y|x)]] \\\\\n",
    "&= \\text{argmin}_\\theta \\mathbb{E}_{\\hat{p}(x)}[\\mathbb{E}_{\\hat{p}(y | x)}[\\text{log}\\hat{p}(y | x)]] - \\mathbb{E}_{\\hat{p}(x)}[\\mathbb{E}_{\\hat{p}(y | x)}[\\text{log}p_\\theta(y|x)]] \\\\\n",
    "&= \\text{argmin}_\\theta \\mathbb{E}_{\\hat{p}(x,y)}[\\text{log}\\hat{p}(y | x)] - \\mathbb{E}_{\\hat{p}(x,y)}[\\text{log}p_\\theta(y|x)] \\\\ \n",
    "&= \\text{argmax}_\\theta \\mathbb{E}_{\\hat{p}(x,y)}[\\text{log}p_\\theta(y|x)] - \\mathbb{E}_{\\hat{p}(x,y)}[\\text{log}\\hat{p}(y | x)] \\\\\n",
    "&= \\text{argmax}_\\theta \\mathbb{E}_{\\hat{p}(x,y)}[\\text{log}p_\\theta(y|x)]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we denote $p_\\theta$ as $p$ and $p_\\gamma$ as $\\hat{p}$.\n",
    "\n",
    "\n",
    "We begin with Bayes rule:\n",
    "\n",
    "$$\n",
    "p(y | x) = \\frac{p(x | y) p(y)}{\\sum_y p(x | y) p(y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $p(x|y)$ can be written in canonical form as:\n",
    "$$\n",
    "p(x |y) = \\exp(\\frac{-1}{2}x^T \\Lambda x + \\eta_y^T x + g_y)\n",
    "$$\n",
    "\n",
    "where,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Lambda &= \\Sigma^{-1} = (\\sigma^2 I)^{-1} \\\\\n",
    "\\eta_y &= \\Lambda \\mu_y \\\\\n",
    "g_y &= \\frac{-1}{2} \\mu_y^T \\Lambda \\mu_y - \\text{log}((2\\pi)^\\frac{n}{2} |\\sigma^2 I|^\\frac{1}{2})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(x | y) p(y) &= \\exp(\\frac{-1}{2}x^T \\Lambda x + \\eta_y^T x + g_y) \\pi_y \\\\\n",
    "&= \\exp(\\frac{-1}{2}x^T \\Lambda x + \\eta_y^T x + g_y + \\text{log}(\\pi_y)) \\\\\n",
    "&= \\exp(\\frac{-1}{2}x^T \\Lambda x) \\exp(\\eta_y^T x + g_y + \\text{log}(\\pi_y))\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging it back to $p(y|x)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y | x) &= \\frac{p(x | y) p(y)}{\\sum_y p(x | y) p(y)} \\\\\n",
    "& = \\frac{\\exp(\\frac{-1}{2}x^T \\Lambda x) \\exp(\\eta_y^T x + g_y + \\text{log}(\\pi_y))}{\\sum_k \\exp(\\frac{-1}{2}x^T \\Lambda x) \\exp(\\eta_k^T x + g_k + \\text{log}(\\pi_k))} \\\\\n",
    "& = \\frac{\\exp(\\eta_y^T x + g_y + \\text{log}(\\pi_y))}{\\sum_k  \\exp(\\eta_k^T x + g_k + \\text{log}(\\pi_k))}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, by setting\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\eta_y &= w_y \\\\\n",
    "g_k + \\text{log}(\\pi_y) &= b_y\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y | x) &= \\frac{\\exp(\\eta_y^T x + g_y + \\text{log}(\\pi_y))}{\\sum_k  \\exp(\\eta_k^T x + g_k + \\text{log}(\\pi_k))} \\\\\n",
    "&= \\frac{\\exp(w_y^T x + b_y)}{\\sum_k \\exp(w_k^T x + b_y)} \\\\\n",
    "&= \\hat{p}(y | x)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "#### Q1\n",
    "It would need $\\prod_i^n k_i -1$ parameters.\n",
    "\n",
    "\n",
    "#### Q2\n",
    "Let $\\mathcal{N} := \\{1,2,\\dots,n\\}$ be the indeces of the topological sort. The chain rule factorization can be written as:\n",
    "$$\n",
    "p(X_1, \\dots, X_n) = \\prod_{i=1}^n p(x_i | \\mathcal{X}_i)\n",
    "$$\n",
    "where $\\mathcal{X}_i = \\{x_j : i-m \\leq j < i, j \\in \\mathcal{N}\\}$.\n",
    "\n",
    "For a given node $p(x_i | \\mathcal{X}_i)$, the number of parameters needed to represent its conditional probability is:\n",
    "$$\n",
    "(k_i - 1) \\prod_{j \\in \\mathcal{X}_i} k_j\n",
    "$$\n",
    "\n",
    "thus, for the entire factorization the total number of parameters is\n",
    "$$\n",
    "\\sum_{i \\in \\mathcal{N}} (k_i - 1) \\prod_{j \\in \\mathcal{X}_i} k_j\n",
    "$$\n",
    "\n",
    "#### Q3\n",
    "When the variables are conditionally independent, the sets $\\{\\mathcal{X}_i\\}_i$ become empty sets, i.e. $\\mathcal{X}_i = \\emptyset$ and the total number of parameters is\n",
    "$$\n",
    "\\sum_{i \\in \\mathcal{N}} (k_i - 1)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "#### Q1\n",
    "\n",
    "The forward and backward models do indeed cover the same hypothesis space. To see this, consider the case where $n = 2$, and note that the factorization described is simply the chain rule in a forward or reverse order:\n",
    "\n",
    "$$\n",
    "p_f(x_1, x_2) = p_f(x_1) p_f(x_2 | x_1) \\\\\n",
    "p_r(x_1, x_2) = p_r(x_2) p_r(x_1 | x_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the canonical form of the gaussians, $\\mathcal{N}(x | \\mu, \\sigma)$:\n",
    "$$\n",
    "p(x) = \\exp(\\Lambda x^2 + \\eta x + g)\n",
    "$$\n",
    "\n",
    "where,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Lambda &= \\Sigma^{-1} = (\\sigma^2 I)^{-1} \\\\\n",
    "\\eta &= \\frac{\\mu}{\\sigma^2}\\\\\n",
    "g &= \\frac{-1}{2 \\sigma^2} \\mu^2 - \\text{log}((2\\pi)^\\frac{1}{2} \\sigma)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We must show that $p_f = p_r$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_f(x_1) p_f(x_2 | x_1) &= p_r(x_2) p_r(x_1 | x_2)\\\\\n",
    "\\mathcal{N}(x_1 | u_1(0), \\sigma_1^2(0)) \\mathcal{N}(x_2 | u_2(x_1), \\sigma_2^2(x_1)) &= \\mathcal{N}(x_2 | \\hat{u}_2(0), \\hat{\\sigma}_2^2(0)) \\mathcal{N}(x_1 | \\hat{u}_1(x_2), \\hat{\\sigma}_1^2(x_2)) \\\\\n",
    "\\exp(\\Lambda_1 x_1^2 + \\eta_1 x_1 + g_1) \\exp(\\Lambda_{2|1} x_2^2 + \\eta_{2|1} x_2 + g_{2|1}) &= \\exp(\\hat{\\Lambda}_2 x_2^2 + \\hat{\\eta}_2 x_2 + \\hat{g}_2) \\exp(\\hat{\\Lambda}_{1|2} x_1^2 + \\hat{\\eta}_{1|2} x_2 + \\hat{g}_{1|2}) \\\\\n",
    "\\Lambda_1 x_1^2 + \\eta_1 x_1 + g_1 + \\Lambda_{2|1} x_2^2 + \\eta_{2|1} x_2 + g_{2|1} &= \\hat{\\Lambda}_2 x_2^2 + \\hat{\\eta}_2 x_2 + \\hat{g}_2 + \\hat{\\Lambda}_{1|2} x_1^2 + \\hat{\\eta}_{1|2} x_2 + \\hat{g}_{1|2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Given $\\Lambda_1, \\eta_1, g_1, \\Lambda_{2|1}, \\eta_{2|1}, g_{2|1}$ and assuming sufficiently powerful neural networks, it is always possible to find $\\Lambda_2, \\eta_2, g_2, \\Lambda_{1|2}, \\eta_{1|2}, g_{1|2}$ such that the last equation holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "#### Q1\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}_{z\\sim p(z)}[\\frac{1}{k}\\sum_k p(x | z)] &= \\frac{1}{k}\\sum_k\\mathbb{E}_{z\\sim p(z)}[p(x | z)] \\\\\n",
    "&= \\frac{1}{k}\\sum_k \\int_z p(x|z)p(z) dz \\\\\n",
    "&= \\frac{1}{k}\\sum_k \\int_z p(x,z) dz \\\\\n",
    "&= \\frac{1}{k}\\sum_k p(x) \\\\\n",
    "&= p(x)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2\n",
    "\n",
    "Since $\\log(x)$ is concave, Jensen's inequality shows that \n",
    "$$\n",
    "\\mathbb{E}[\\log(x)] \\leq \\log(\\mathbb{E}[x])\n",
    "$$\n",
    "thus\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}_{z\\sim p(z)}[\\log(\\frac{1}{k}\\sum_k p(x | z))] &\\leq \\log(\\mathbb{E}_{z\\sim p(z)}[\\frac{1}{k}\\sum_k p(x | z)]) \\\\\n",
    "&= \\log(p(x))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, $\\log A$ is _not_ an unbiased estimator of $\\log p(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1\n",
    "The number 656 in binary is $1010010000$. Which requires 10 digits. Thus, $n = 10$ should be sufficient to represnt all 657 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2\n",
    "In binary, we can represent up to 1024 numbers using 10 digits. Thus, increasing the character alphabet from 657 to 900 would not increase the number of parameters $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3-Q5\n",
    "Submitted separately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
